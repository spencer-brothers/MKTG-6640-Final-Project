---
title: Restaurant Reviews Analysis
output: 
  html_document: 
    toc: true
---

# Introduction

Customer reviews serve as a powerful tool for business decision-making. By analyzing review data, restaurants can gain a deeper understanding of the competitive landscape, gauge public sentiment, identify emerging trends, assess pricing strategies across the market, and even refine their location strategies. In short, review data empowers restaurants to make informed, data-driven decisions that enhance both operational effectiveness and customer satisfaction.


## Business Problem

Online reviews on sites like Yelp offer valuable insights, but there are too many for restaurant owners or marketers to read manually. Analyzing them at scale helps businesses make smarter decisions, like adjusting menus, improving service, or refining marketing. Research shows customer reviews strongly influence both strategy and appeal. This project simulates a real-world consulting scenario where we act as analytics advisors for a restaurant group or a city’s tourism department. By applying text analytics to Yelp reviews, we’ll uncover common sentiments and themes to guide business decisions. Text data reveals patterns in market trends, competition, and operations, turning raw feedback into strategic insight.

## Data Description

This dataset was collected by scraping Yelp’s search results for top-rated restaurants in Los Angeles. A Python script using the BeautifulSoup library was used to extract relevant information directly from the HTML of Yelp pages. The final dataset includes 240 entries, each representing a top-recommended restaurant in LA. For each restaurant, the data includes details such as star ratings, review counts, cuisine type, price range, and user comments.

[Link to dataset](https://www.kaggle.com/datasets/lorentzyeung/top-240-recommended-restaurants-in-la-2023?resource=download&select=top+240+restaurants+recommanded+in+los+angeles+URL.csv)

### Features

- **Rank**: The restaurant’s position in Yelp’s list of top recommendations.
- **CommentDate**: The date a customer posted their comment.
- **Date**: When the data was scraped.
- **RestaurantName**: The name of the restaurant.
- **Comment**: A user-submitted review or comment about the restaurant.
- **Address**: The restaurant’s location.
- **StarRating**: The average rating (out of 5 stars) based on user reviews.
- **NumberOfReviews**: Total number of reviews the restaurant has received.
- **Style**: The type of cuisine served (e.g., Italian, Japanese).
- **Price**: The general price range, represented with dollar signs (\$ to \$\$\$\$).

## Notebook Approach

In the sections that follow, we’ll walk through a structured analysis of Yelp reviews for Los Angeles' top-rated restaurants. We’ll begin with data cleaning and exploratory analysis to understand patterns in ratings, prices, and review volume. From there, we’ll shift focus to the customer comments—preprocessing the text and applying two powerful topic modeling techniques: Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA). These methods allow us to uncover the key themes and hidden structures within the reviews, providing insights into what customers consistently care about, such as service, ambiance, or food quality.

# Setup

```{r Setup}
# Set seed for reproducibility
set.seed(42)

# Import packages
install.packages("quanteda")
install.packages("tidytext")
install.packages("tm")
install.packages("text2vec")
install.packages("wordcloud")
install.packages("topicmodels")
install.packages("irlba")
if (!require(wordVectors)) {
  if (!(require(devtools))) {
    install.packages("devtools")
  }
  devtools::install_github("bmschmidt/wordVectors")
}

library(wordVectors)
library(tidyverse)
library(dplyr)
library(quanteda)
library(tidytext)
library(lubridate)
library(tm)
library(slam)
library(text2vec)
library(wordcloud)
library(topicmodels)
library(irlba)
library(parallel)
library(caret)

# Load raw datafile
# restaurant_raw <- read_csv("top 240 restaurants recommanded in los angeles 2.csv")

restaurant_raw <- read_csv("C:/Users/burrb/OneDrive/Documents/R/top 240 restaurants recommanded in los angeles 2.csv")

summary(restaurant_raw)
```

The date column only has one value, the date the data were collected from Yelp. We will remove this column and convert the restaurant name, address, and price columns to factors. 

```{r Preliminary Cleaning}
restaurant_raw <- restaurant_raw |>
  select(-Date,-Price) |>
  mutate(
    RestaurantName = as.factor(RestaurantName)
  )
# summary statistics
summary(restaurant_raw)
```

# Exploratory Data Analysis

```{r eda}
# table of counts by Restaurant style
top_styles <- sort(table(restaurant_raw$Style), decreasing = TRUE)
# see top 5 
head(top_styles, 5)

# dist of prices, including those with no price
table(restaurant_raw$Price, useNA = "ifany")

# how many unique restaurants
length(unique(restaurant_raw$RestaurantName))

# how many NA's by column
colSums(is.na(restaurant_raw))

# Distribution of star ratings
restaurant_raw |>
  ggplot(aes(x = StarRating)) +
  geom_histogram(binwidth = .25) +
  labs(title = "Distribution of Star Ratings",
       x = "Star Rating",
       y = "Count")

# Price
# ggplot(restaurant_raw, aes(x = fct_explicit_na(Price, na_level = "Unknown"))) +
#   geom_bar() +
#   labs(title = "Counts of Price Ranges",
#       x = "Price Range")

# Star rating vs. Number of reviews
ggplot(restaurant_raw, aes(x = factor(StarRating), y = NumberOfReviews)) +
  geom_boxplot() +
  labs(title = "Number of Reviews by Star Rating",
       x = "Star Rating",
       y = "Number of Reviews")

# price by star rating
# ggplot(restaurant_raw, aes(x = StarRating, y = Price)) +
#   geom_boxplot() +
#   labs(title = "Star Rating by Price",
#        x = "Star Rating",
#        y = "Price")

# number of reviews by price
# ggplot(restaurant_raw, aes(x = Price, y = NumberOfReviews)) +
#   geom_boxplot() +
#   labs(title = "Number of Reviews by Price",
#        x = "Price",
#        y = "Number of Reviews")

# count of comments over time
restaurant_raw %>%
  mutate(month = floor_date(CommentDate, "month")) %>%
  count(month) %>%
  ggplot(aes(x = month, y = n)) +
  geom_line() +
  labs(title = "Comments Over Time", x = "Month", y = "Count")
# only last 10 reviews for restaurant scraped, explaining the large spike at end

# heatmap of number of comments by day of week and month
restaurant_raw %>%
  filter(!is.na(CommentDate)) %>%
  mutate(
    Weekday = wday(CommentDate, label = TRUE, abbr = TRUE),
    Month = month(CommentDate, label = TRUE, abbr = TRUE)
  ) %>%
  count(Month, Weekday) %>%
  ggplot(aes(x = Weekday, y = Month, fill = n)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightyellow", high = "red") +
  labs(
    title = "Heatmap of Comments by Day of Week and Month",
    x = "Day of Week",
    y = "Month",
    fill = "Number of Comments"
  ) +
  theme_minimal()
```

# Text Preprocessing

```{r pre-processing}
# Convert Comments to tokens
restaurant_raw_tokens <- tokens(restaurant_raw$Comment,
                               what="word", remove_numbers=T,
                               remove_punct=T, remove_symbols=T) |>
  tokens_remove(stopwords("en")) |>
  tokens_tolower()

# Create DFM, removing terms which occur in less than 1% of all docs and more than 90% of all docs.
restaurant_raw_dfm <- restaurant_raw_tokens  |>
  dfm() |>
  dfm_trim(min_docfreq=0.01, max_docfreq=0.9, docfreq_type="prop")
```

## Word Embeddings

```{r word2vec Preprocessing}
# Convert the comments to a raw text file
restaurant_text <- restaurant_raw$Comment
writeLines(restaurant_text, "restaurant_corpus.txt")

# Preprocess Corpus
prep_word2vec("restaurant_corpus.txt", 
              destination = "restaurant_corpus_clean.txt", 
              lowercase = TRUE, 
              alphabets = "latin", 
              removeNumbers = TRUE, 
              removePunctuation = TRUE)

num_cores = detectCores()
```

```{r word2vec Training}

restaurant_model <- train_word2vec(
  train_file = "restaurant_corpus_clean.txt",
  output_file = "restaurant_vectors.bin",
  vectors = 100,         # embedding size
  window = 5,            # context window size
  min_count = 5,         # ignore words that appear fewer than 5 times
  threads = num_cores - 1,           # use multiple CPU cores
  iter = 10              # number of training iterations
)

# Load the embeddings for use
restaurant_embeds <- read.vectors("restaurant_vectors.bin")

# Test embeddings
closest_to(restaurant_embeds, restaurant_embeds[["pizza"]], n = 10)
```


# LDA

LDA (Latent Dirichlet Allocation) is a topic modeling technique that helps uncover hidden themes in large sets of text data. Applying it to restaurant comments allows us to identify common topics—like service quality, cuisine types, or ambiance—without reading every review manually. This provides a scalable way to summarize customer feedback and understand what people are consistently saying about LA’s top restaurants.

Let's start with finding our optimal k value.

## Finding K
```{r optimize k}
dtm_sparse <- convert(restaurant_raw_dfm, to = "topicmodels")

# Remove sparse terms to reduce dimensionality
# dtm_sparse <- removeSparseTerms(dtm_converted, 0.98)

# Remove empty rows from DTM
row_totals <- apply(dtm_sparse, 1, sum)
dtm_filtered <- dtm_sparse[row_totals > 0, ]

#remotes::install_github("nikita-moor/ldatuning")
library(ldatuning)

# Compute the K value "scores" from K=2 to K=25
result <- FindTopicsNumber(
    dtm_filtered,
    topics = seq(from = 2, to = 25, by = 1),
    metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
    method = "Gibbs",#Gibbs can used too
    control = list(seed = 1971),
    mc.cores = 2L,
    return_models= TRUE,
    verbose = TRUE
)

FindTopicsNumber_plot(result)
```

We will use 25 topics because that appears to be the optimal number of topics based on the ldatuning model. 

## Run LDA
```{r run lda}
# Run LDA
lda_model <- LDA(dtm_filtered, k = 25, control = list(seed = 1234))

# View top 10 terms in each topic
terms(lda_model, 10)
```

It looks like the most important topics are positive and centered around service, location and ambiance. 

Let's visualize these topics to gain a little more clarity.

## Visualizing topics
```{r visualize topics, fig.width = 15}
# Tidy format of topics and terms
topics <- tidy(lda_model, matrix = "beta")

# Get top 10 terms per topic
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Plot top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(
    title = "Top 10 Terms per LDA Topic",
    x = NULL,
    y = "Term Importance (Beta)"
  )
```

The LDA has shown us that most of the reviews focus on the service, location and ambiance but it doesn't give us a lot of actional insight. To get better insights, we will now look at an LSA and some predictive analytics to try to extract what kind of service or what location drives the most positive reviews. 

# LSA

```{r lsa, fig.heat = 12, fig.width = 12}
# Preprocess the text data: Clean and preprocess the reviews
clean_text <- function(text) {
  text <- tolower(text)  # Convert to lowercase
  text <- removePunctuation(text)  # Remove punctuation
  text <- removeNumbers(text)  # Remove numbers
  text <- removeWords(text, stopwords("en"))  # Remove stopwords
  text <- stripWhitespace(text)  # Remove extra spaces
  return(text)
}

# Apply text cleaning to the 'Comment' column
restaurant_raw$Cleaned_Comment <- sapply(restaurant_raw$Comment, clean_text)

# Tokenize and vectorize the text using TF-IDF
# Create a vocabulary and tokenizer
it <- itoken(restaurant_raw$Cleaned_Comment, progressbar = TRUE)

# Create a vocabulary and a document-term matrix (DTM)
vectorizer <- vocab_vectorizer(vocabulary = create_vocabulary(it))
dtm <- create_dtm(it, vectorizer)

# Apply TF-IDF transformation
tfidf <- TfIdf$new()
tfidf_matrix <- tfidf$fit_transform(dtm)

# Perform LSA by applying Singular Value Decomposition (SVD)
svd_model <- irlba(tfidf_matrix, nv = 5)  # 5 topics (adjust based on the data size)
topics_matrix <- svd_model$u %*% diag(svd_model$d)

# Inspect the topics (top words per topic)
terms <- colnames(tfidf_matrix)

# Function to extract top terms for each topic
topic_terms <- function(matrix, terms, top_n = 10) {
  result <- list()
  for (i in 1:ncol(matrix)) {
    topic_terms <- order(matrix[, i], decreasing = TRUE)[1:top_n]
    result[[i]] <- terms[topic_terms]
  }
  return(result)
}

# Get the top terms for each topic
top_words <- topic_terms(topics_matrix, terms)
print(top_words)

# Example: Assign topics to each review based on maximum value
topic_assignment <- apply(topics_matrix, 1, function(row) which.max(row))
restaurant_raw$Assigned_Topic <- topic_assignment

# Summary of topic distribution
topic_counts <- table(restaurant_raw$Assigned_Topic)
print(topic_counts)

# terms for each topic
topic_word_matrix <- svd_model$v %*% diag(svd_model$d)
rownames(topic_word_matrix) <- colnames(tfidf_matrix)


# Word cloud visualization for topics
for (i in 1:ncol(topic_word_matrix)) {
  word_freq <- sort(topic_word_matrix[, i], decreasing = TRUE)[1:50]
  word_freq <- word_freq[!is.na(word_freq) & word_freq > 0]
  wordcloud(names(word_freq), freq = word_freq, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))
}

# Visualizing the topic distribution for each restaurant
ggplot(restaurant_raw, aes(x = as.factor(Assigned_Topic))) +
  geom_bar() +
  labs(title = "Topic Distribution Across Reviews", x = "Topic", y = "Number of Reviews")

```

To analyze the key themes in customer feedback, we applied Latent Semantic Analysis (LSA). After cleaning and preprocessing the review text which involved removing punctuation, numbers, stop words, and standardizing the text, the clean comments were transformed using TF-IDF vectorization, which weights terms based on their importance in the context of all reviews. Singular Value Decomposition (SVD) was then used to reduce the dimensionality of this matrix and extract five latent topics, each representing a cluster of frequently co-occurring words. Each review was assigned to the topic it aligned with most strongly. The topic distribution revealed that Topic 2 dominated overwhelmingly, with over 2,000 reviews assigned to it, while the remaining topics had very few reviews. This indicates that one central theme is driving the majority of customer sentiment. Word clouds generated for each topic show that the most frequently mentioned words include “great,” “amazing,” “food,” and “service,” suggesting that the dominant topic likely reflects overall positive dining experiences, with customers praising food quality and service. This insight helps highlight what customers value most and underscores the importance of consistently delivering excellent food and service in the restaurant industry.

```{r predictive analytics}
# Get vocabulary from trained embeddings
embedding_vocab <- rownames(restaurant_embeds)

# Tokenize and average word embeddings per comment
get_comment_embedding <- function(comment, embeddings, vocab) {
  # Match prep_word2vec processing: lowercase + remove numbers/punctuation + keep latin
  comment <- tolower(comment)
  comment <- gsub("[^a-z\\s]", " ", comment)  # only keep letters and spaces
  tokens <- unlist(strsplit(comment, "\\s+"))
  tokens <- tokens[tokens %in% vocab]
  
  if (length(tokens) == 0) {
    return(rep(0, ncol(embeddings)))
  } else {
    return(colMeans(embeddings[tokens, , drop = FALSE]))
  }
}

# Apply to all comments
comment_vectors <- t(sapply(restaurant_raw$Comment, get_comment_embedding,
                            embeddings = restaurant_embeds,
                            vocab = embedding_vocab))

# Convert to data frame
comment_df <- as.data.frame(comment_vectors)
comment_df$StarRating <- restaurant_raw$StarRating 

# separate into train and test
train_index <- sample(nrow(comment_df), 0.8 * nrow(comment_df))
train_data <- comment_df[train_index, ]
test_data <- comment_df[-train_index, ]

library(randomForest)
rf_model <- randomForest(StarRating ~ ., data = train_data, ntree = 250, mtry = sqrt(ncol(train_data)))
preds <- predict(rf_model, newdata = train_data)

sqrt(mean((preds - train_data$StarRating)^2))

preds_test <- predict(rf_model, newdata = test_data)
sqrt(mean((preds_test - test_data$StarRating)^2))

residuals <- preds - test_data$StarRating
sst <- sum((test_data$StarRating - mean(test_data$StarRating))^2)
sse <- sum(residuals^2)
r_squared <- 1 - (sse / sst)

# hypertuning
mtry_vals <- c(2, sqrt(ncol(train_data)), ncol(train_data) / 2)
ntree_vals <- c(100, 250, 500)

results <- data.frame(mtry = numeric(), ntree = numeric(), RMSE = numeric())

for (m in mtry_vals) {
  for (n in ntree_vals) {
    set.seed(123)
    rf_model <- randomForest(
      StarRating ~ ., data = train_data,
      mtry = floor(m), ntree = n
    )
    preds <- predict(rf_model, newdata = test_data)
    rmse <- sqrt(mean((preds - test_data$StarRating)^2))
    results <- rbind(results, data.frame(mtry = m, ntree = n, RMSE = rmse))
  }
}

results[order(results$RMSE), ]

# support vector machine
library(e1071)
# manual hypertuning for svm to find best values
cost_vals <- c(0.1, 1, 10)
gamma_vals <- c(0.01, 0.1, 1)
epsilon_vals <- c(0.1, 0.2, 0.3)

results <- data.frame(cost = numeric(), gamma = numeric(), epsilon = numeric(), RMSE = numeric())

for (c in cost_vals) {
  for (g in gamma_vals) {
    for (e in epsilon_vals) {
      model <- svm(
        StarRating ~ ., data = train_data,
        kernel = "radial", cost = c, gamma = g, epsilon = e
      )
      preds <- predict(model, newdata = test_data)
      rmse <- sqrt(mean((preds - test_data$StarRating)^2))
      
      results <- rbind(results, data.frame(cost = c, gamma = g, epsilon = e, RMSE = rmse))
    }
  }
}

results[order(results$RMSE), ]

# best model parameters
svm_model <- svm(StarRating ~ ., data = train_data, kernel = "radial", cost = 1, gamma = .01, epsilon = 0.3)

svm_train_preds <- predict(svm_model, newdata = train_data)
# RMSE for training data
sqrt(mean((svm_train_preds - train_data$StarRating)^2))

# Predict on test set
svm_preds <- predict(svm_model, newdata = test_data)
sqrt(mean((svm_preds - test_data$StarRating)^2))

svm_residuals <- svm_preds - test_data$StarRating
svm_sst <- sum((test_data$StarRating - mean(test_data$StarRating))^2)
svm_sse <- sum(svm_residuals^2)
svm_r_squared <- 1 - (svm_sse / svm_sst)

svm_r_squared

## low R squared, .3, tough to determine ratings just through text. 
# language doesn't explain much rating variation

lm_model <- lm(StarRating ~., data = train_data)

lm_train_preds <- predict(lm_model, newdata = train_data)
sqrt(mean((lm_train_preds - train_data$StarRating)^2))

lm_preds <- predict(lm_model, newdata = test_data)
sqrt(mean((lm_preds - test_data$StarRating)^2))

plot(test_data$StarRating, lm_preds,
     xlab = "Actual Star Rating",
     ylab = "Predicted Star Rating",
     main = "Actual vs. Predicted",
     pch = 16, col = rgb(0,0,1,0.3))

# glmnet
library(glmnet)
X <- as.matrix(train_data[, -which(names(train_data) == "StarRating")])
y <- train_data$StarRating

X_test <- as.matrix(test_data[, -which(names(test_data) == "StarRating")])
y_test <- test_data$StarRating


ridge_model <- cv.glmnet(X, y, alpha = 0)
best_lambda_ridge <- ridge_model$lambda.min


lasso_model <- cv.glmnet(X, y, alpha = 1)
best_lambda_lasso <- lasso_model$lambda.min

# Ridge Predictions
ridge_preds <- predict(ridge_model, s = best_lambda_ridge, newx = X_test)
ridge_rmse <- sqrt(mean((ridge_preds - y_test)^2))

# Lasso Predictions
lasso_preds <- predict(lasso_model, s = best_lambda_lasso, newx = X_test)
lasso_rmse <- sqrt(mean((lasso_preds - y_test)^2))

elastic_model <- cv.glmnet(X, y, alpha = .5)
best_lambda_elastic <- elastic_model$lambda.min

elastic_preds <- predict(elastic_model, s = best_lambda_elastic, newx = X_test)
elastic_rmse <- sqrt(mean((elastic_preds - y_test)^2))
elastic_rmse


sqrt(mean((lm_preds - y_test)^2))
lm_residuals <- lm_preds - test_data$StarRating
lm_sst <- sum((test_data$StarRating - mean(test_data$StarRating))^2)
lm_sse <- sum(lm_residuals^2)
lm_r_squared <- 1 - (lm_sse / lm_sst)

lm_r_squared

## SVM model performed best so far
plot(test_data$StarRating, svm_preds, 
     xlab = "Actual", ylab = "Predicted", 
     main = "Actual vs. Predicted Star Ratings")

```

```{r tfidf predictions}
topics_df <- as.data.frame(topics_matrix)
topics_df$StarRating <- restaurant_raw$StarRating

train_indices <- sample(seq_len(nrow(topics_df)), size = 0.8 * nrow(topics_df))
train_data <- topics_df[train_indices, ]
test_data <- topics_df[-train_indices, ]

lm_model <- lm(StarRating ~ ., data = train_data)

preds <- predict(lm_model, newdata = test_data)
sqrt(mean((preds - test_data$StarRating)^2))


X <- as.matrix(train_data[, -which(names(train_data) == "StarRating")])
y <- train_data$StarRating

X_test <- as.matrix(test_data[, -which(names(test_data) == "StarRating")])
y_test <- test_data$StarRating


ridge_model <- cv.glmnet(X, y, alpha = 0)
best_lambda_ridge <- ridge_model$lambda.min


lasso_model <- cv.glmnet(X, y, alpha = 1)
best_lambda_lasso <- lasso_model$lambda.min

# Ridge Predictions
ridge_preds <- predict(ridge_model, s = best_lambda_ridge, newx = X_test)
ridge_rmse <- sqrt(mean((ridge_preds - y_test)^2))

# Lasso Predictions
lasso_preds <- predict(lasso_model, s = best_lambda_lasso, newx = X_test)
lasso_rmse <- sqrt(mean((lasso_preds - y_test)^2))

elastic_model <- cv.glmnet(X, y, alpha = .5)
best_lambda_elastic <- elastic_model$lambda.min

elastic_preds <- predict(elastic_model, s = best_lambda_elastic, newx = X_test)
elastic_rmse <- sqrt(mean((elastic_preds - y_test)^2))
elastic_rmse
ridge_rmse
lasso_rmse

svm_model <- svm(StarRating ~ ., data = train_data, kernel = "radial", cost = 1, gamma = .01, epsilon = 0.3)
svm_train_preds <- predict(svm_model, newdata = train_data)
# RMSE for training data
sqrt(mean((svm_train_preds - train_data$StarRating)^2))

# Predict on test set
svm_preds <- predict(svm_model, newdata = test_data)
sqrt(mean((svm_preds - test_data$StarRating)^2))

cost_vals <- c(0.1, 1, 10)
gamma_vals <- c(0.01, 0.1, 1)
epsilon_vals <- c(0.1, 0.2, 0.3)

results <- data.frame(cost = numeric(), gamma = numeric(), epsilon = numeric(), RMSE = numeric())

for (c in cost_vals) {
  for (g in gamma_vals) {
    for (e in epsilon_vals) {
      model <- svm(
        StarRating ~ ., data = train_data,
        kernel = "radial", cost = c, gamma = g, epsilon = e
      )
      preds <- predict(model, newdata = test_data)
      rmse <- sqrt(mean((preds - test_data$StarRating)^2))
      
      results <- rbind(results, data.frame(cost = c, gamma = g, epsilon = e, RMSE = rmse))
    }
  }
}

summary(lm_model)
```
