---
title: Restaurant Reviews Analysis
output: 
  html_document: 
    toc: true
---

# Introduction

Customer reviews serve as a powerful tool for business decision-making. By analyzing review data, restaurants can gain a deeper understanding of the competitive landscape, gauge public sentiment, identify emerging trends, assess pricing strategies across the market, and even refine their location strategies. In short, review data empowers restaurants to make informed, data-driven decisions that enhance both operational effectiveness and customer satisfaction.


## Business Problem

Online reviews on sites like Yelp offer valuable insights, but there are too many for restaurant owners or marketers to read manually. Analyzing them at scale helps businesses make smarter decisions, like adjusting menus, improving service, or refining marketing. Research shows customer reviews strongly influence both strategy and appeal. This project simulates a real-world consulting scenario where we act as analytics advisors for a restaurant group or a city’s tourism department. By applying text analytics to Yelp reviews, we’ll uncover common sentiments and themes to guide business decisions. Text data reveals patterns in market trends, competition, and operations, turning raw feedback into strategic insight.

## Data Description

This dataset was collected by scraping Yelp’s search results for top-rated restaurants in Los Angeles. A Python script using the BeautifulSoup library was used to extract relevant information directly from the HTML of Yelp pages. The final dataset includes 240 entries, each representing a top-recommended restaurant in LA. For each restaurant, the data includes details such as star ratings, review counts, cuisine type, price range, and user comments.

[Link to dataset](https://www.kaggle.com/datasets/lorentzyeung/top-240-recommended-restaurants-in-la-2023?resource=download&select=top+240+restaurants+recommanded+in+los+angeles+URL.csv)

### Features

- **Rank**: The restaurant’s position in Yelp’s list of top recommendations.
- **CommentDate**: The date a customer posted their comment.
- **Date**: When the data was scraped.
- **RestaurantName**: The name of the restaurant.
- **Comment**: A user-submitted review or comment about the restaurant.
- **Address**: The restaurant’s location.
- **StarRating**: The average rating (out of 5 stars) based on user reviews.
- **NumberOfReviews**: Total number of reviews the restaurant has received.
- **Style**: The type of cuisine served (e.g., Italian, Japanese).
- **Price**: The general price range, represented with dollar signs (\$ to \$\$\$\$).

# Setup

```{r Setup}
# Set seed for reproducibility
set.seed(42)

# Import packages
#install.packages("quanteda")
#install.packages("tidytext")
library(tidyverse)
library(dplyr)
library(quanteda)
library(tidytext)
library(lubridate)
library(tm)
library(slam)
library(text2vec)
library(wordcloud)
library(topicmodels)
library(irlba)

# Load raw datafile
restaurant_raw <- read_csv("top 240 restaurants recommanded in los angeles 2.csv")

summary(restaurant_raw)
```

The date column only has one value, the date the data were collected from Yelp. We will remove this column and convert the restaurant name, address, and price columns to factors. 

```{r Preliminary Cleaning}
restaurant_raw <- restaurant_raw |>
  select(-Date) |>
  mutate(
    RestaurantName = as.factor(RestaurantName),
    Price = as.factor(Price)
  )
# summary statistics
summary(restaurant_raw)
```

# Exploratory Data Analysis

```{r eda}
# table of counts by Restaurant style
top_styles <- sort(table(restaurant_raw$Style), decreasing = TRUE)
# see top 5 
head(top_styles, 5)

# dist of prices, including those with no price
table(restaurant_raw$Price, useNA = "ifany")

# how many unique restaurants
length(unique(restaurant_raw$RestaurantName))

# how many NA's by column
colSums(is.na(restaurant_raw))

# Distribution of star ratings
restaurant_raw |>
  ggplot(aes(x = StarRating)) +
  geom_histogram(binwidth = .25) +
  labs(title = "Distribution of Star Ratings",
       x = "Star Rating",
       y = "Count")

# Price
ggplot(restaurant_raw, aes(x = fct_explicit_na(Price, na_level = "Unknown"))) +
  geom_bar() +
  labs(title = "Counts of Price Ranges",
       x = "Price Range")

# Star rating vs. Number of reviews
ggplot(restaurant_raw, aes(x = factor(StarRating), y = NumberOfReviews)) +
  geom_boxplot() +
  labs(title = "Number of Reviews by Star Rating",
       x = "Star Rating",
       y = "Number of Reviews")

# price by star rating
ggplot(restaurant_raw, aes(x = StarRating, y = Price)) +
  geom_boxplot() +
  labs(title = "Star Rating by Price",
       x = "Star Rating",
       y = "Price")

# number of reviews by price
ggplot(restaurant_raw, aes(x = Price, y = NumberOfReviews)) +
  geom_boxplot() +
  labs(title = "Number of Reviews by Price",
       x = "Price",
       y = "Number of Reviews")

# count of comments over time
restaurant_raw %>%
  mutate(month = floor_date(CommentDate, "month")) %>%
  count(month) %>%
  ggplot(aes(x = month, y = n)) +
  geom_line() +
  labs(title = "Comments Over Time", x = "Month", y = "Count")
# only last 10 reviews for restaurant scraped, explaining the large spike at end

# heatmap of number of comments by day of week and month
restaurant_raw %>%
  filter(!is.na(CommentDate)) %>%
  mutate(
    Weekday = wday(CommentDate, label = TRUE, abbr = TRUE),
    Month = month(CommentDate, label = TRUE, abbr = TRUE)
  ) %>%
  count(Month, Weekday) %>%
  ggplot(aes(x = Weekday, y = Month, fill = n)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightyellow", high = "red") +
  labs(
    title = "Heatmap of Comments by Day of Week and Month",
    x = "Day of Week",
    y = "Month",
    fill = "Number of Comments"
  ) +
  theme_minimal()
```

# Text Preprocessing

```{r pre-processing}
# Convert Comments to tokens
restaurant_raw_tokens <- tokens(restaurant_raw$Comment,
                               what="word", remove_numbers=T,
                               remove_punct=T, remove_symbols=T) |>
  #tokens_remove(stopwords("en", source="snowball")) |>
  tokens_tolower()

# Create DFM, removing terms which occur in less than 1% of all docs and more than 90% of all docs.
restaurant_raw_dfm <- restaurant_raw_tokens  |>
  dfm() |>
  dfm_trim(min_docfreq=0.01, max_docfreq=0.9, docfreq_type="prop")
```


# LDA

LDA (Latent Dirichlet Allocation) is a topic modeling technique that helps uncover hidden themes in large sets of text data. Applying it to restaurant comments allows us to identify common topics—like service quality, cuisine types, or ambiance—without reading every review manually. This provides a scalable way to summarize customer feedback and understand what people are consistently saying about LA’s top restaurants.

## LDA pre-processing

Let's create a corpus for our LDA, clean the text and generate a document term matrix for analysis.

```{r lda prep}
# Create a corpus from the Comment column
corpus <- VCorpus(VectorSource(restaurant_raw$Comment))

# Clean the text
corpus <- corpus %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace)

# Create a Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)

# Remove sparse terms to reduce dimensionality
dtm_sparse <- removeSparseTerms(dtm, 0.98)
```

Now that the data is prepped, let's run LDA with 3 topics.

## Running LDA
```{r lda with 3 topics}
# Run LDA with 3 topics
k <- 3

# Remove empty rows from DTM
row_totals <- apply(dtm_sparse, 1, sum)
dtm_filtered <- dtm_sparse[row_totals > 0, ]

# Run LDA
lda_model <- LDA(dtm_filtered, k = k, control = list(seed = 1234))

# View top 10 terms in each topic
terms(lda_model, 10)
```
It looks like the most important topics are positive and centered around service, location and ambiance. 

Let's visualize these topics to gain a little more clarity.

## Visualizing topics
```{r visualize topics}
# Tidy format of topics and terms
library(tidytext)
topics <- tidy(lda_model, matrix = "beta")

# Get top 10 terms per topic
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Plot top terms for each topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(
    title = "Top 10 Terms per LDA Topic",
    x = NULL,
    y = "Term Importance (Beta)"
  )
```

# LSA

```{r lsa, fig.heat = 10, fig.width = 10}
# Preprocess the text data: Clean and preprocess the reviews
clean_text <- function(text) {
  text <- tolower(text)  # Convert to lowercase
  text <- removePunctuation(text)  # Remove punctuation
  text <- removeNumbers(text)  # Remove numbers
  text <- removeWords(text, stopwords("en"))  # Remove stopwords
  text <- stripWhitespace(text)  # Remove extra spaces
  return(text)
}

# Apply text cleaning to the 'Comment' column
restaurant_raw$Cleaned_Comment <- sapply(restaurant_raw$Comment, clean_text)

# Tokenize and vectorize the text using TF-IDF
# Create a vocabulary and tokenizer
it <- itoken(restaurant_raw$Cleaned_Comment, progressbar = TRUE)

# Create a vocabulary and a document-term matrix (DTM)
vectorizer <- vocab_vectorizer(vocabulary = create_vocabulary(it))
dtm <- create_dtm(it, vectorizer)

# Apply TF-IDF transformation
tfidf <- TfIdf$new()
tfidf_matrix <- tfidf$fit_transform(dtm)

# Perform LSA by applying Singular Value Decomposition (SVD)
svd_model <- irlba(tfidf_matrix, nv = 5)  # 5 topics (adjust based on the data size)
topics_matrix <- svd_model$u %*% diag(svd_model$d)

# Inspect the topics (top words per topic)
terms <- colnames(tfidf_matrix)

# Function to extract top terms for each topic
topic_terms <- function(matrix, terms, top_n = 10) {
  result <- list()
  for (i in 1:ncol(matrix)) {
    topic_terms <- order(matrix[, i], decreasing = TRUE)[1:top_n]
    result[[i]] <- terms[topic_terms]
  }
  return(result)
}

# Get the top terms for each topic
top_words <- topic_terms(topics_matrix, terms)
print(top_words)

# Example: Assign topics to each review based on maximum value
topic_assignment <- apply(topics_matrix, 1, function(row) which.max(row))
restaurant_raw$Assigned_Topic <- topic_assignment

# Summary of topic distribution
topic_counts <- table(restaurant_raw$Assigned_Topic)
print(topic_counts)

# terms for each topic
topic_word_matrix <- svd_model$v %*% diag(svd_model$d)
rownames(topic_word_matrix) <- colnames(tfidf_matrix)


# Word cloud visualization for topics
for (i in 1:ncol(topic_word_matrix)) {
  word_freq <- sort(topic_word_matrix[, i], decreasing = TRUE)[1:50]
  word_freq <- word_freq[!is.na(word_freq) & word_freq > 0]
  wordcloud(names(word_freq), freq = word_freq, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))
}

# Visualizing the topic distribution for each restaurant
ggplot(restaurant_raw, aes(x = as.factor(Assigned_Topic))) +
  geom_bar() +
  labs(title = "Topic Distribution Across Reviews", x = "Topic", y = "Number of Reviews")

```


